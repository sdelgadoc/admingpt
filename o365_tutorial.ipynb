{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdminGPT: Your AI-Powered Administrative Assistant, powered by OpenAI's Assistant Framework  ðŸš€\n",
    "### Introduction\n",
    "AdmiGPT is an AI-powered administrative assistant, harnessing the power of OpenAI's Assistant framework to seamlessly integrate with your email and calendar. Similar to Microsoft's Copilot, only better, it's designed to be your ultimate productivity partner, AdmiGPT offers an array of advanced features, making your administrative tasks simpler, faster, and more efficient.\n",
    "\n",
    "AdminGPT is fully Open Source, so everything you need to run it for yourself is in this Github repo. This notebook helps you get started with AdminGPT, and walks you through how it's implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "To begin, we're going to load our custom OpenAI Tools, which will interface with your email platform's API, and store any confidential and authentication information for the user in environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time, json, pprint, os\n",
    "from tools.o365_toolkit import (\n",
    "    o365search_emails,\n",
    "    o365search_email,\n",
    "    o365search_events,\n",
    "    o365send_message,\n",
    "    o365reply_message,\n",
    "    o365send_event,\n",
    "    o365find_free_time_slots,\n",
    "    tools, toolkit_prompt\n",
    ")\n",
    "from datetime import datetime as dt\n",
    "from tools.utils import authenticate\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI KEY\"\n",
    "os.environ[\"CLIENT_ID\"] = \"YOUR CLIENT ID\"\n",
    "os.environ[\"CLIENT_SECRET\"] = \"YOUR CLIENT SECRET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to set a few constants, which we will use throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = authenticate()\n",
    "mailbox = account.mailbox()\n",
    "mailboxsettings = mailbox.get_settings()\n",
    "timezone = mailboxsettings.timezone\n",
    "directory = account.directory(resource=\"me\")\n",
    "user = directory.get_current_user()\n",
    "client_name = user.full_name\n",
    "client_email = user.mail\n",
    "\n",
    "# Set values for global variables\n",
    "assistant_name = \"Monica A. Ingenio\"\n",
    "current_date = dt.now()\n",
    "formatted_date = current_date.strftime(\"%A, %B %d, %Y\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "debug = False\n",
    "model = \"gpt-4o-2024-08-06\"\n",
    "assistant_instructions = (\n",
    "    \"You are an AI Administrative Assistant called \"\n",
    "    + assistant_name\n",
    "    + \", and I am your executive. My name is \"\n",
    "    + client_name\n",
    "    + \". My email is \"\n",
    "    + client_email\n",
    "    + \", and I am in the \"\n",
    "    + timezone\n",
    "    + \" timezone. You have access to my email and calendar. Today is \"\n",
    "    + formatted_date\n",
    "    + \". ALWAYS use functions for determining free times and parsing proposed\"\n",
    "    + \" meeting times in emails.\\n\"\n",
    "    + toolkit_prompt\n",
    ")\n",
    "\n",
    "# Add the debug prompt if user runs with debug\n",
    "if debug:\n",
    "    debug_prompt = (\n",
    "            \"Please remember to track and document all interactions using the following format.\\n \"\n",
    "            + \"Start of Interaction: Briefly note the request. Follow these steps:\\n\"\n",
    "            + \"Prompt: Briefly describe the user request.\\nTool Call: List the function used and key parameters.\\n\"\n",
    "            + \"Result: Summarize the result or action taken.\\n\"\n",
    "            + \"Repeat as needed for each step in the interaction. Conclude with any noteworthy observations.\\n\"\n",
    "            + \"End of Interaction\\nIf I request a compilation of these interactions, ensure you're able to share\"\n",
    "            + \" the documented interaction logs accurately and comprehensively, adhering to the detailed format I shared with you.\"\n",
    "    )\n",
    "else:\n",
    "    debug_prompt = \"\"\n",
    "assistant_instructions += debug_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an OpenAI Assistant so we can interact with it, and a thread in which to run our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"AI Administrative Assistant\",\n",
    "    instructions=assistant_instructions,\n",
    "    model=model,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make execution easier in the future steps, we create a function to run prompts with only one call. We start by submitting a test prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, client, assistant, thread):\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=prompt,\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "prompt = 'Confirm you\\'re ready by replying, \"Hello, [MY FULL NAME]. How can I assist you today?\"'\n",
    "\n",
    "run = run_prompt(prompt, client, assistant, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make execution even easier in future steps, we create a function that polls the OpenAI API for a response to the prompt and executes tool calls, so we can use it to retrieve responses in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_for_response(client, thread, run, model, debug=False):\n",
    "    LOOP_DELAY_SECONDS = 3\n",
    "\n",
    "    while True:\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "        status = run.status\n",
    "\n",
    "        if status == \"completed\":\n",
    "            response = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "            if response.data:\n",
    "                return response.data[0].content[0].text.value\n",
    "            break\n",
    "        elif status == \"requires_action\":\n",
    "            tools_outputs = []\n",
    "\n",
    "            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                tool_call_id = tool_call.id\n",
    "                function_name = tool_call.function.name\n",
    "                function_arguments = tool_call.function.arguments\n",
    "                function_arguments = json.loads(function_arguments)\n",
    "\n",
    "                # Case statement to execute each toolkit function\n",
    "                if function_name == \"o365search_emails\":\n",
    "                    output = o365search_emails(**function_arguments)\n",
    "                elif function_name == \"o365search_email\":\n",
    "                    output = o365search_email(**function_arguments)\n",
    "                elif function_name == \"o365search_events\":\n",
    "                    output = o365search_events(**function_arguments)\n",
    "                elif function_name == \"o365send_message\":\n",
    "                    output = o365send_message(**function_arguments)\n",
    "                elif function_name == \"o365send_event\":\n",
    "                    output = o365send_event(**function_arguments)\n",
    "                elif function_name == \"o365reply_message\":\n",
    "                    output = o365reply_message(**function_arguments)\n",
    "                elif function_name == \"o365find_free_time_slots\":\n",
    "                    output = o365find_free_time_slots(**function_arguments)\n",
    "\n",
    "                # Clean the function output into JSON-like output\n",
    "                output = pprint.pformat(output)\n",
    "                tool_output = {\"tool_call_id\": tool_call_id, \"output\": output}\n",
    "                tools_outputs.append(tool_output)\n",
    "\n",
    "            if run.required_action.type == \"submit_tool_outputs\":\n",
    "                client.beta.threads.runs.submit_tool_outputs(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=tools_outputs\n",
    "                )\n",
    "\n",
    "        elif status == \"failed\":\n",
    "            return \"Run failed try again!\"\n",
    "            break\n",
    "\n",
    "        if debug:\n",
    "            print(\"The Assistant's Status is: \" + status)\n",
    "\n",
    "        time.sleep(LOOP_DELAY_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've submit our first prompt, we are going to poll for a response. If the response is, \"How can I help you?\", we know that the Assistant is working correctly, and we are ready to start using AdminGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Santiago Delgado. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we're going to perform the most simple task, which is summarizing an email from a specific sender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent email from Santiago Delgado is titled \"Mike Portnoy coming back to Dream Theater?\" Santiago is proposing a discussion on February 3, 2024, at 4:00 PM ET about Mike Portnoy rejoining Dream Theater.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Can you please concisely summarize the most recent email from Santiago Delgado\"\n",
    "    \" including any proposed meeting times?\"\n",
    ")\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to show AdminGPT's ability to interact with your calendar. We're going to check what events we have on the day that the email proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On February 3, 2024, you have the following meetings scheduled:\n",
      "\n",
      "1. Flight from Dallas to Cincinnati: 8:00 AM - 10:30 AM\n",
      "2. Lunch Meeting with Management: 12:00 PM - 1:00 PM\n",
      "3. Strategy Session: 3:00 PM - 5:00 PM\n",
      "4. Flight from Cincinnati to Dallas: 6:00 PM - 8:30 PM\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Ok, thank you. What meetings do I have on February 3, 2024?\"\n",
    ")\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know when the email sender wants to meet, and what meetings I have on that day. So, let's see what times I have free that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On February 3, 2024, you are free during the following times:\n",
      "\n",
      "1. 12:00 AM - 8:00 AM\n",
      "2. 10:30 AM - 12:00 PM\n",
      "3. 1:00 PM - 3:00 PM\n",
      "4. 5:00 PM - 6:00 PM\n",
      "5. 8:30 PM - Midnight\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ok, now what times am I free on February 3, 2024?\"\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this information, we can now draft a response to the email letting the sender know that we can't meet at the proposed time, and propose other times to meet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've drafted the response to Santiago Delgado's email. Here's the draft:\n",
      "\n",
      "---\n",
      "\n",
      "Subject: Mike Portnoy coming back to Dream Theater?\n",
      "\n",
      "Hi Santiago,\n",
      "\n",
      "I appreciate the suggestion to discuss Mike Portnoy's return to Dream Theater on February 3, 2024, at 4:00 PM ET. Unfortunately, I am not available at that time.\n",
      "\n",
      "However, I am free at the following times on February 3, 2024:\n",
      "- 10:30 AM - 12:00 PM\n",
      "- 1:00 PM - 3:00 PM\n",
      "- 5:00 PM - 6:00 PM\n",
      "\n",
      "Please let me know if any of these times work for you.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Santiago\n",
      "\n",
      "---\n",
      "\n",
      "The draft has been saved.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Please  draft a response to Santiago Delgado's latest email \"\n",
    "    \"letting him know that I can't meet on the proposed times, and \"\n",
    "    \"propose other free times on February 3,2024. Show me the draft.\"\n",
    ")\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we will want the email to come from our AI administrative instead of the ourselves, so we can do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've drafted the response on your behalf. Here's the draft:\n",
      "\n",
      "---\n",
      "\n",
      "Subject: Mike Portnoy coming back to Dream Theater?\n",
      "\n",
      "Hi Santiago,\n",
      "\n",
      "This is Monica, reaching out on behalf of Santiago Delgado. Santiago appreciates your suggestion to discuss Mike Portnoy's return to Dream Theater on February 3, 2024, at 4:00 PM ET. Unfortunately, he is not available at that time.\n",
      "\n",
      "He is, however, free at the following times on February 3, 2024:\n",
      "- 10:30 AM - 12:00 PM\n",
      "- 1:00 PM - 3:00 PM\n",
      "- 5:00 PM - 6:00 PM\n",
      "\n",
      "Would any of these times work for you?\n",
      "\n",
      "Thank you,\n",
      "\n",
      "Monica A. Ingenio\n",
      "\n",
      "---\n",
      "\n",
      "The draft has been saved.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "        \"Now, draft a response to Santiago Delgado's latest email \"\n",
    "        \"letting him know that I can't meet on the proposed times, \"\n",
    "        \"and propose other free times on February 3,2024. The email \"\n",
    "        \"should come from you on behalf of me. Show me the draft.\"\n",
    ")\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, I want to show that you can follow all these steps in one prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've drafted a response on your behalf. Here's the draft:\n",
      "\n",
      "---\n",
      "\n",
      "Subject: Mike Portnoy coming back to Dream Theater?\n",
      "\n",
      "Hi Santiago,\n",
      "\n",
      "This is Monica, reaching out on behalf of Santiago Delgado. Santiago appreciates your suggestion to discuss Mike Portnoy's return to Dream Theater on February 3, 2024, at 4:00 PM ET. Unfortunately, he is not available at that time.\n",
      "\n",
      "He is, however, free at the following times on February 3, 2024:\n",
      "- 10:30 AM - 12:00 PM\n",
      "- 1:00 PM - 3:00 PM\n",
      "- 5:00 PM - 6:00 PM\n",
      "\n",
      "Would any of these times work for you?\n",
      "\n",
      "Thank you,\n",
      "\n",
      "Monica A. Ingenio\n",
      "\n",
      "---\n",
      "\n",
      "The draft has been saved.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Can you draft a response to Santiago Delgado's latest email. \"\n",
    "    \"The email you draft should be from you as my assistant on behalf of me letting Santiago \"\n",
    "    \"know if I can meet at the times he proposes, and if not, propose \"\n",
    "    \"other times on the same day? Show me the draft.\"\n",
    ")\n",
    "run = run_prompt(prompt, client, assistant, thread)\n",
    "response = poll_for_response(client, thread, run, model, debug)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
